{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import natsort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = natsort.natsorted(glob.glob(r'C:\\Users\\Koumeyl\\Downloads\\test\\*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.24  Python-3.10.2 torch-1.13.1+cpu CPU\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=C:\\Users\\Koumeyl\\Projects\\Koumeyl-YOLOv8_Computer_Vision_Project\\data.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, cache=False, device=, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=False, val=True, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, save_dir=c:\\Users\\Koumeyl\\Projects\\Koumeyl-YOLOv8_Computer_Vision_Project\\runs\\detect\\train11\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "\nDataset 'C:\\Users\\Koumeyl\\Projects\\Koumeyl-YOLOv8_Computer_Vision_Project\\data.yaml' not found ⚠️, missing paths ['C:\\\\Users\\\\Koumeyl\\\\Projects\\\\datasets\\\\valid\\\\images']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m \u001b[39mimport\u001b[39;00m YOLO\n\u001b[0;32m      3\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39myolov8n.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(data\u001b[39m=\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mKoumeyl\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mProjects\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mKoumeyl-YOLOv8_Computer_Vision_Project\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdata.yaml\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \u001b[39m# train the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# results = model.val()  # evaluate model performance on the validation set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Koumeyl\\Projects\\Koumeyl-YOLOv8_Computer_Vision_Project\\env\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:202\u001b[0m, in \u001b[0;36mYOLO.train\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m overrides\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mresume\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    200\u001b[0m     overrides[\u001b[39m\"\u001b[39m\u001b[39mresume\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt_path\n\u001b[1;32m--> 202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mTrainerClass(overrides\u001b[39m=\u001b[39;49moverrides)\n\u001b[0;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m overrides\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mresume\u001b[39m\u001b[39m\"\u001b[39m):  \u001b[39m# manually set model only if not resuming\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mget_model(weights\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, cfg\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39myaml)\n",
      "File \u001b[1;32mc:\\Users\\Koumeyl\\Projects\\Koumeyl-YOLOv8_Computer_Vision_Project\\env\\lib\\site-packages\\ultralytics\\yolo\\engine\\trainer.py:121\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, cfg, overrides)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdata\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.yaml\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m check_det_dataset(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\n\u001b[0;32m    122\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mclassify\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m check_cls_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\Koumeyl\\Projects\\Koumeyl-YOLOv8_Computer_Vision_Project\\env\\lib\\site-packages\\ultralytics\\yolo\\data\\utils.py:237\u001b[0m, in \u001b[0;36mcheck_det_dataset\u001b[1;34m(dataset, autodownload)\u001b[0m\n\u001b[0;32m    235\u001b[0m     LOGGER\u001b[39m.\u001b[39mwarning(msg)\n\u001b[0;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(msg)\n\u001b[0;32m    238\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    239\u001b[0m \u001b[39mif\u001b[39;00m s\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mhttp\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m s\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.zip\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# URL\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: \nDataset 'C:\\Users\\Koumeyl\\Projects\\Koumeyl-YOLOv8_Computer_Vision_Project\\data.yaml' not found ⚠️, missing paths ['C:\\\\Users\\\\Koumeyl\\\\Projects\\\\datasets\\\\valid\\\\images']"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "results = model.train(data=r\"C:\\Users\\Koumeyl\\Projects\\Koumeyl-YOLOv8_Computer_Vision_Project\\data.yaml\")  # train the model\n",
    "# results = model.val()  # evaluate model performance on the validation set\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.24  Python-3.10.2 torch-1.13.1+cpu CPU\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "0: 384x640 12 persons, 1: 384x640 12 persons, 2: 384x640 12 persons, 1 sports ball, 3: 384x640 13 persons, 4: 384x640 11 persons, 2 sports balls, 5: 384x640 11 persons, 1 chair, 6: 384x640 11 persons, 7: 384x640 11 persons, 8: 384x640 11 persons, 9: 384x640 12 persons, 10: 384x640 13 persons, 11: 384x640 11 persons, 12: 384x640 9 persons, 13: 384x640 10 persons, 14: 384x640 11 persons, 15: 384x640 10 persons, 16: 384x640 11 persons, 17: 384x640 12 persons, 1 sports ball, 18: 384x640 12 persons, 1 sports ball, 19: 384x640 13 persons, 1 sports ball, 20: 384x640 12 persons, 1 sports ball, 21: 384x640 11 persons, 1 sports ball, 22: 384x640 12 persons, 2 sports balls, 23: 384x640 14 persons, 1 sports ball, 24: 384x640 10 persons, 25: 384x640 15 persons, 26: 384x640 14 persons, 27: 384x640 12 persons, 28: 384x640 17 persons, 29: 384x640 20 persons, 30: 384x640 17 persons, 31: 384x640 14 persons, 32: 384x640 16 persons, 33: 384x640 14 persons, 34: 384x640 15 persons, 35: 384x640 17 persons, 36: 384x640 16 persons, 37: 384x640 16 persons, 38: 384x640 16 persons, 39: 384x640 15 persons, 40: 384x640 16 persons, 41: 384x640 17 persons, 42: 384x640 20 persons, 1 sports ball, 43: 384x640 14 persons, 1 sports ball, 44: 384x640 16 persons, 2 sports balls, 45: 384x640 19 persons, 1 sports ball, 46: 384x640 21 persons, 1 sports ball, 47: 384x640 18 persons, 1 sports ball, 48: 384x640 17 persons, 1 sports ball, 49: 384x640 18 persons, 1 sports ball, 50: 384x640 14 persons, 51: 384x640 16 persons, 1 sports ball, 52: 384x640 17 persons, 1 sports ball, 53: 384x640 15 persons, 54: 384x640 20 persons, 1 sports ball, 55: 384x640 19 persons, 1 sports ball, 56: 384x640 17 persons, 1 sports ball, 57: 384x640 17 persons, 1 sports ball, 58: 384x640 18 persons, 1 sports ball, 59: 384x640 15 persons, 60: 384x640 16 persons, 61: 384x640 16 persons, 1 sports ball, 62: 384x640 13 persons, 1 sports ball, 63: 384x640 13 persons, 64: 384x640 17 persons, 65: 384x640 18 persons, 66: 384x640 13 persons, 1 sports ball, 67: 384x640 13 persons, 68: 384x640 15 persons, 1 sports ball, 69: 384x640 17 persons, 70: 384x640 18 persons, 71: 384x640 16 persons, 72: 384x640 14 persons, 73: 384x640 16 persons, 1 sports ball, 74: 384x640 15 persons, 1 car, 1 sports ball, 75: 384x640 19 persons, 1 sports ball, 76: 384x640 17 persons, 1 sports ball, 77: 384x640 13 persons, 1 sports ball, 78: 384x640 12 persons, 1 sports ball, 79: 384x640 13 persons, 1 sports ball, 80: 384x640 19 persons, 2 sports balls, 81: 384x640 14 persons, 2 sports balls, 82: 384x640 17 persons, 3 sports balls, 83: 384x640 14 persons, 1 sports ball, 84: 384x640 14 persons, 85: 384x640 12 persons, 86: 384x640 15 persons, 2 sports balls, 87: 384x640 14 persons, 1 sports ball, 88: 384x640 15 persons, 89: 384x640 15 persons, 90: 384x640 14 persons, 91: 384x640 14 persons, 92: 384x640 12 persons, 93: 384x640 13 persons, 94: 384x640 12 persons, 1 sports ball, 95: 384x640 14 persons, 96: 384x640 14 persons, 97: 384x640 15 persons, 98: 384x640 13 persons, 1 sports ball, 99: 384x640 12 persons, 100: 384x640 12 persons, 101: 384x640 11 persons, 102: 384x640 13 persons, 103: 384x640 14 persons, 104: 384x640 13 persons, 105: 384x640 12 persons, 1 sports ball, 106: 384x640 12 persons, 107: 384x640 12 persons, 1 sports ball, 108: 384x640 12 persons, 109: 384x640 12 persons, 110: 384x640 13 persons, 111: 384x640 13 persons, 112: 384x640 13 persons, 113: 384x640 10 persons, 114: 384x640 9 persons, 115: 384x640 10 persons, 1 sports ball, 116: 384x640 11 persons, 117: 384x640 10 persons, 118: 384x640 9 persons, 119: 384x640 9 persons, 120: 384x640 11 persons, 121: 384x640 12 persons, 122: 384x640 11 persons, 123: 384x640 12 persons, 124: 384x640 11 persons, 125: 384x640 8 persons, 9412.4ms\n",
      "Speed: 0.7ms pre-process, 74.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mc:\\Users\\Koumeyl\\Projects\\Koumeyl-YOLOv8_Computer_Vision_Project\\runs\\detect\\predict8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dict = model(\n",
    "   source=input,\n",
    "   conf=0.25,\n",
    "   save=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set()\n",
    "for result in dict:\n",
    "    s.add(result.boxes.shape)\n",
    "    if result.boxes.shape[0] == 30:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50b14529a2a34ee29b1f622af3b90b53971f5e50a2ea2dbb489d135c3188c0fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
